{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNN - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AlexNet  (2012)\n",
    "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\n",
    "* \"ImageNet Classification with Deep Convolutional Neural Networks\"\n",
    "* Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton created a “large, deep convolutional neural network” that was used to win the 2012 ILSVRC (ImageNet Large-Scale Visual Recognition Challenge).\n",
    "* 2012 marked the first year where a CNN was used to achieve a top 5 test error rate of 15.4% \n",
    "* The next best entry achieved an error of 26.2%, which was an astounding improvement that pretty much shocked the computer vision community.\n",
    "* 1000 categories\n",
    "![](https://world4jason.gitbooks.io/research-log/content/deepLearning/CNN/Model%20&%20ImgNet/alexnet/img/alexnet2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Trained the network on ImageNet data, which contained over 15 million annotated images from a total of over 22,000 categories.\n",
    "* Used ReLU for the nonlinearity functions.\n",
    "* Used data augmentation techniques that consisted of image translations, horizontal reflections, and patch extractions.\n",
    "* Implemented dropout layers in order to combat the problem of overfitting to the training data.\n",
    "* Trained the model using batch stochastic gradient descent, with specific values for momentum and weight decay.\n",
    "* Trained on two GTX 580 GPUs for five to six days.\n",
    "\n",
    "The neural network developed by Krizhevsky, Sutskever, and Hinton in 2012 was the coming out party for CNNs in the computer vision community. This was the first time a model performed so well on a historically difficult ImageNet dataset. Utilizing techniques that are still used today, such as data augmentation and dropout, this paper really illustrated the benefits of CNNs and backed them up with record breaking performance in the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ZF Net (2013)\n",
    "https://arxiv.org/pdf/1311.2901v3.pdf\n",
    "* “Visualizing and Understanding Convolutional Neural Networks”\n",
    "* Matthew Zeiler and Rob Fergus\n",
    "* 11.2% error rate\n",
    "* fine tuning to the previous AlexNet structure\n",
    "* explaining a lot of the intuition behind ConvNets and showing how to visualize the filters and weights correctly\n",
    "\n",
    "![](https://adeshpande3.github.io/assets/zfnet.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Very similar architecture to AlexNet, except for a few minor modifications.\n",
    "* AlexNet trained on 15 million images, while ZF Net trained on only 1.3 million images.\n",
    "* Instead of using 11x11 sized filters in the first layer (which is what AlexNet implemented), ZF Net used filters of size 7x7 and a decreased stride value. The reasoning behind this modification is that a smaller filter size in the first conv layer helps retain a lot of original pixel information in the input volume. A filtering of size 11x11 proved to be skipping a lot of relevant information, especially as this is the first conv layer.\n",
    "* As the network grows, we also see a rise in the number of filters used.\n",
    "* Used ReLUs for their activation functions, cross-entropy loss for the error function, and trained using batch stochastic gradient descent.\n",
    "* Trained on a GTX 580 GPU for twelve days.\n",
    "* Developed a visualization technique named Deconvolutional Network, which helps to examine different feature activations and their relation to the input space. Called “deconvnet” because it maps features to pixels (the opposite of what a convolutional layer does).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DeConvNet\n",
    "\n",
    "The basic idea behind how this works is that at every layer of the trained CNN, you attach a “deconvnet” which has a path back to the image pixels. An input image is fed into the CNN and activations are computed at each level. This is the forward pass. Now, let’s say we want to examine the activations of a certain feature in the 4th conv layer. We would store the activations of this one feature map, but set all of the other activations in the layer to 0, and then pass this feature map as the input into the deconvnet. This deconvnet has the same filters as the original CNN. This input then goes through a series of unpool (reverse maxpooling), rectify, and filter operations for each preceding layer until the input space is reached.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://adeshpande3.github.io/assets/deconvnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://adeshpande3.github.io/assets/deconvnet2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# VGG Net (2014)\n",
    "https://arxiv.org/pdf/1409.1556v6.pdf\n",
    "* \"VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION\"\n",
    "* Karen Simonyan and Andrew Zisserman\n",
    "* 7.3% error rate.\n",
    "* Simplicity and depth. 19 layer CNN that strictly used 3x3 filters with stride and pad of 1, along with 2x2 maxpooling layers with stride 2.\n",
    "![](https://adeshpande3.github.io/assets/VGGNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The use of only 3x3 sized filters is quite different from AlexNet’s 11x11 filters in the first layer and ZF Net’s 7x7 filters. The authors’ reasoning is that the combination of two 3x3 conv layers has an effective receptive field of 5x5. This in turn simulates a larger filter while keeping the benefits of smaller filter sizes. One of the benefits is a decrease in the number of parameters. Also, with two conv layers, we’re able to use two ReLU layers instead of one.\n",
    "* 3 conv layers back to back have an effective receptive field of 7x7.\n",
    "* As the spatial size of the input volumes at each layer decrease (result of the conv and pool layers), the depth of the volumes increase due to the increased number of filters as you go down the network.\n",
    "* Interesting to notice that the number of filters doubles after each maxpool layer. This reinforces the idea of shrinking spatial dimensions, but growing depth.\n",
    "* Worked well on both image classification and localization tasks. The authors used a form of localization as regression (see page 10 of the paper for all details).\n",
    "* Built model with the Caffe toolbox.\n",
    "* Used scale jittering as one data augmentation technique during training.\n",
    "* Used ReLU layers after each conv layer and trained with batch gradient descent.\n",
    "* Trained on 4 Nvidia Titan Black GPUs for two to three weeks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GoogLeNet (2015)\n",
    "https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf\n",
    "* \"Going Deeper with Convolutions\"\n",
    "* No simplicity - Inception module.\n",
    "* 22 layers\n",
    "* ILSVRC 2014 winner with a top 5 error rate of 6.7%\n",
    "* Refference: [1] Know your meme: We need to go deeper. http://knowyourmeme.com/memes/we-need-to-go-deeper.\n",
    "![](http://i0.kym-cdn.com/photos/images/original/000/531/557/a88.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://adeshpande3.github.io/assets/GoogleNet.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://adeshpande3.github.io/assets/GoogLeNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inception Module\n",
    "![](https://adeshpande3.github.io/assets/GoogLeNet3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The bottom green box is our input and the top one is the output of the model (Turning this picture right 90 degrees would let you visualize the model in relation to the last picture which shows the full network). Basically, at each layer of a traditional ConvNet, you have to make a choice of whether to have a pooling operation or a conv operation (there is also the choice of filter size). What an Inception module allows you to do is perform all of these operations in parallel. In fact, this was exactly the “naïve” idea that the authors came up with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://adeshpande3.github.io/assets/GoogLeNet4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, why doesn’t this work? It would lead to way too many outputs. We would end up with an extremely large depth channel for the output volume. The way that the authors address this is by adding 1x1 conv operations before the 3x3 and 5x5 layers. The 1x1 convolutions (or network in network layer) provide a method of dimensionality reduction. For example, let’s say you had an input volume of 100x100x60 (This isn’t necessarily the dimensions of the image, just the input to any layer of the network). Applying 20 filters of 1x1 convolution would allow you to reduce the volume to 100x100x20. This means that the 3x3 and 5x5 convolutions won’t have as large of a volume to deal with. This can be thought of as a “pooling of features” because we are reducing the depth of the volume, similar to how we reduce the dimensions of height and width with normal maxpooling layers.  Another note is that these 1x1 conv layers are followed by ReLU units which definitely can’t hurt (See Aaditya Prakash’s great post for more info on the effectiveness of 1x1 convolutions). Check out this video for a great visualization of the filter concatenation at the end.\n",
    "\n",
    "You may be asking yourself “How does this architecture help?”. Well, you have a module that consists of a network in network layer, a medium sized filter convolution, a large sized filter convolution, and a pooling operation. The network in network conv is able to extract information about the very fine grain details in the volume, while the 5x5 filter is able to cover a large receptive field of the input, and thus able to extract its information as well. You also have a pooling operation that helps to reduce spatial sizes and combat overfitting. On top of all of that, you have ReLUs after each conv layer, which help improve the nonlinearity of the network. Basically, the network is able to perform the functions of these different operations while still remaining computationally considerate. The paper does also give more of a high level reasoning that involves topics like sparsity and dense connections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Main Points\n",
    "\n",
    "* Used 9 Inception modules in the whole architecture, with over 100 layers in total! Now that is deep…\n",
    "* No use of fully connected layers! They use an average pool instead, to go from a 7x7x1024 volume to a 1x1x1024 volume. This saves a huge number of parameters.\n",
    "* Uses 12x fewer parameters than AlexNet.\n",
    "* During testing, multiple crops of the same image were created, fed into the network, and the softmax probabilities were averaged to give us the final solution.\n",
    "* Utilized concepts from R-CNN for their detection model.\n",
    "* There are updated versions to the Inception module (Versions 6 and 7).\n",
    "* Trained on “a few high-end GPUs within a week”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why It’s Important\n",
    "\n",
    "GoogLeNet was one of the first models that introduced the idea that CNN layers didn’t always have to be stacked up sequentially. Coming up with the Inception module, the authors showed that a creative structuring of layers can lead to improved performance and computationally efficiency. This paper has really set the stage for some amazing architectures that we could see in the coming years.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Microsoft ResNet (2015)\n",
    "https://arxiv.org/pdf/1512.03385v1.pdf\n",
    "* \"Deep Residual Learning for Image Recognition\"\n",
    "* 152 layer architecture \n",
    "* Set new records in classification, detection, and localization through one incredible architecture.\n",
    "* ILSVRC 2015 with an incredible error rate of 3.6% (Depending on their skill and expertise, humans generally hover around a 5-10% error rate.\n",
    "\n",
    "![](https://adeshpande3.github.io/assets/ResNet.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Residual Block\n",
    "\n",
    "The idea behind a residual block is that you have your input x go through conv-relu-conv series. This will give you some F(x). That result is then added to the original input x. Let’s call that H(x) = F(x) + x. In traditional CNNs, your H(x) would just be equal to F(x) right? So, instead of just computing that transformation (straight from x to F(x)), we’re computing the term that you have to add, F(x), to your input, x. Basically, the mini module shown below is computing a “delta” or a slight change to the original input x to get a slightly altered representation (When we think of traditional CNNs, we go from x to F(x) which is a completely new representation that doesn’t keep any information about the original x). The authors believe that “it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping”.\n",
    "\n",
    "Another reason for why this residual block might be effective is that during the backward pass of backpropagation, the gradient will flow easily through the graph because we have addition operations, which distributes the gradient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://adeshpande3.github.io/assets/ResNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Main Points\n",
    "\n",
    "* “Ultra-deep” – Yann LeCun.\n",
    "* 152 layers…\n",
    "* Interesting note that after only the first 2 layers, the spatial size gets compressed from an input volume of 224x224 to a 56x56 volume.\n",
    "* Authors claim that a naïve increase of layers in plain nets result in higher training and test error (Figure 1 in the paper).\n",
    "* The group tried a 1202-layer network, but got a lower test accuracy, presumably due to overfitting.\n",
    "* Trained on an 8 GPU machine for two to three weeks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Object Localization\n",
    "\n",
    "* Additional 4 outputs: `x`, `y`, `width`, `height`\n",
    "* Custom loss function using softmax and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Object Detection\n",
    "https://medium.com/@nikasa1889/the-modern-history-of-object-recognition-infographic-aea18517c318\n",
    "![](https://cdn-images-1.medium.com/max/2000/1*xAd4dHFESXxGm40IdsHP1Q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Style Transfer\n",
    "https://arxiv.org/pdf/1508.06576.pdf (2015)\n",
    "![](https://i.imgur.com/gG4fvAC.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generating Image Descriptions (2014)\n",
    "https://arxiv.org/pdf/1412.2306v2.pdf\n",
    "* \"Deep Visual-Semantic Alignments for Generating Image Descriptions\"\n",
    "* Andrej Karpathy, Li Fei-Fei\n",
    "* Combining CNN & bi-directional LSTM to generate natural language descriptions of different image regions. \n",
    "* Input an image - outputs a text description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](https://adeshpande3.github.io/assets/Caption.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Adversarial Networks \n",
    "https://arxiv.org/pdf/1312.6199v4.pdf\n",
    "* Intriguing properties of neural networks (2014)\n",
    "\n",
    "![](https://adeshpande3.github.io/assets/Adversarial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples\n",
    "https://keras.io/applications/#applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-01-09 13:15:34--  https://i.ytimg.com/vi/SNggmeilXDQ/maxresdefault.jpg\n",
      "Resolving i.ytimg.com (i.ytimg.com)... 209.85.202.101, 209.85.202.138, 209.85.202.139, ...\n",
      "Connecting to i.ytimg.com (i.ytimg.com)|209.85.202.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 118853 (116K) [image/jpeg]\n",
      "Saving to: ‘maxresdefault.jpg.1’\n",
      "\n",
      "maxresdefault.jpg.1 100%[===================>] 116.07K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2018-01-09 13:15:34 (4.27 MB/s) - ‘maxresdefault.jpg.1’ saved [118853/118853]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://i.ytimg.com/vi/SNggmeilXDQ/maxresdefault.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Class Prediction using ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
      "24576/35363 [===================>..........] - ETA: 0sPredicted: [('n01871265', 'tusker', 0.55347723), ('n02504013', 'Indian_elephant', 0.3473224), ('n02504458', 'African_elephant', 0.093237929)]\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "img_path = 'maxresdefault.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](maxresdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extract features from an arbitrary intermediate layer with VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 143,667,240\n",
      "Trainable params: 143,667,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "base_model = VGG19(weights='imagenet')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\n",
    "\n",
    "img_path = 'maxresdefault.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "block4_pool_features = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14, 14, 512)\n",
      "[[[[    0.             0.           122.54925537 ...,   177.70880127\n",
      "      531.36535645     0.        ]\n",
      "   [  139.80073547   105.62182617     0.         ...,     0.            28.69142914\n",
      "        0.        ]\n",
      "   [  218.14257812     0.             0.         ...,     0.           320.17297363\n",
      "        0.        ]\n",
      "   ..., \n",
      "   [  323.05130005     0.             0.         ...,     0.             0.\n",
      "        0.        ]\n",
      "   [  505.80566406     0.             0.         ...,     0.             0.\n",
      "        0.        ]\n",
      "   [   68.26963806     0.             0.         ...,     0.           363.43035889\n",
      "        0.        ]]\n",
      "\n",
      "  [[   43.93622971     0.            20.30366325 ...,     0.           317.33456421\n",
      "        0.        ]\n",
      "   [  352.26470947     0.             0.         ...,     0.           208.03858948\n",
      "        0.        ]\n",
      "   [  514.26416016     0.            64.01551819 ...,     0.           491.24716187\n",
      "        0.        ]\n",
      "   ..., \n",
      "   [  251.37638855     0.            64.99979401 ...,  1078.25561523\n",
      "      153.43206787     0.        ]\n",
      "   [  469.35650635     0.             7.40624285 ...,     0.           440.97027588\n",
      "        0.        ]\n",
      "   [  169.08496094     0.             0.         ...,     0.             0.\n",
      "        0.        ]]\n",
      "\n",
      "  [[    0.             0.             0.         ...,     0.             0.\n",
      "      210.23703003]\n",
      "   [   90.42609406     0.            61.89510345 ...,   114.12887573     0.\n",
      "       89.93547821]\n",
      "   [  163.7958374     94.26544189    66.4033432  ...,   104.34303284     0.\n",
      "      125.18013   ]\n",
      "   ..., \n",
      "   [    0.             0.             0.         ...,   491.42773438     0.\n",
      "        0.        ]\n",
      "   [  231.61131287     0.             0.         ...,   104.05222321     0.\n",
      "        0.        ]\n",
      "   [  321.99502563     0.             0.         ...,     0.             0.\n",
      "        0.        ]]\n",
      "\n",
      "  ..., \n",
      "  [[    0.             0.             0.         ...,     0.             0.\n",
      "        0.        ]\n",
      "   [    0.             0.           224.06518555 ...,     0.             0.\n",
      "        0.        ]\n",
      "   [   57.30466843     0.             0.         ...,     0.             0.\n",
      "      104.63116455]\n",
      "   ..., \n",
      "   [    0.            52.42150497     0.         ...,     0.             0.\n",
      "        0.        ]\n",
      "   [    0.             0.             0.         ...,     0.             0.\n",
      "        0.        ]\n",
      "   [  209.14796448     0.             0.         ...,     0.             0.\n",
      "        0.        ]]\n",
      "\n",
      "  [[    0.             0.           109.9282608  ...,     0.             0.\n",
      "      217.73460388]\n",
      "   [    0.             0.             0.         ...,    74.03837585     0.\n",
      "        0.        ]\n",
      "   [    0.             0.             0.         ...,   375.40673828     0.\n",
      "        0.        ]\n",
      "   ..., \n",
      "   [    0.             0.             0.         ...,   153.57034302     0.\n",
      "        0.        ]\n",
      "   [    0.             0.             0.         ...,   861.41790771     0.\n",
      "        0.        ]\n",
      "   [  501.52288818     0.             0.         ...,     0.             0.\n",
      "        0.        ]]\n",
      "\n",
      "  [[    0.           422.22711182     0.         ...,   100.96258545\n",
      "      363.83013916     0.        ]\n",
      "   [    0.           548.48620605     0.         ...,   170.02937317\n",
      "      348.87850952     0.        ]\n",
      "   [    0.           675.27142334    52.00751495 ...,     0.             0.\n",
      "        0.        ]\n",
      "   ..., \n",
      "   [    0.             0.             0.         ...,   455.44384766     0.\n",
      "        0.        ]\n",
      "   [    0.             0.             0.         ...,   746.07312012     0.\n",
      "      192.56748962]\n",
      "   [  376.0262146     55.38940811     0.         ...,     0.             0.\n",
      "      204.31973267]]]]\n"
     ]
    }
   ],
   "source": [
    "print(block4_pool_features.shape)\n",
    "print(block4_pool_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fine-tune InceptionV3 on a new set of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87654400/87910968 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(200, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "# model.fit_generator(...)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_7\n",
      "1 conv2d_1\n",
      "2 batch_normalization_1\n",
      "3 activation_99\n",
      "4 conv2d_2\n",
      "5 batch_normalization_2\n",
      "6 activation_100\n",
      "7 conv2d_3\n",
      "8 batch_normalization_3\n",
      "9 activation_101\n",
      "10 max_pooling2d_3\n",
      "11 conv2d_4\n",
      "12 batch_normalization_4\n",
      "13 activation_102\n",
      "14 conv2d_5\n",
      "15 batch_normalization_5\n",
      "16 activation_103\n",
      "17 max_pooling2d_4\n",
      "18 conv2d_9\n",
      "19 batch_normalization_9\n",
      "20 activation_107\n",
      "21 conv2d_7\n",
      "22 conv2d_10\n",
      "23 batch_normalization_7\n",
      "24 batch_normalization_10\n",
      "25 activation_105\n",
      "26 activation_108\n",
      "27 average_pooling2d_1\n",
      "28 conv2d_6\n",
      "29 conv2d_8\n",
      "30 conv2d_11\n",
      "31 conv2d_12\n",
      "32 batch_normalization_6\n",
      "33 batch_normalization_8\n",
      "34 batch_normalization_11\n",
      "35 batch_normalization_12\n",
      "36 activation_104\n",
      "37 activation_106\n",
      "38 activation_109\n",
      "39 activation_110\n",
      "40 mixed0\n",
      "41 conv2d_16\n",
      "42 batch_normalization_16\n",
      "43 activation_114\n",
      "44 conv2d_14\n",
      "45 conv2d_17\n",
      "46 batch_normalization_14\n",
      "47 batch_normalization_17\n",
      "48 activation_112\n",
      "49 activation_115\n",
      "50 average_pooling2d_2\n",
      "51 conv2d_13\n",
      "52 conv2d_15\n",
      "53 conv2d_18\n",
      "54 conv2d_19\n",
      "55 batch_normalization_13\n",
      "56 batch_normalization_15\n",
      "57 batch_normalization_18\n",
      "58 batch_normalization_19\n",
      "59 activation_111\n",
      "60 activation_113\n",
      "61 activation_116\n",
      "62 activation_117\n",
      "63 mixed1\n",
      "64 conv2d_23\n",
      "65 batch_normalization_23\n",
      "66 activation_121\n",
      "67 conv2d_21\n",
      "68 conv2d_24\n",
      "69 batch_normalization_21\n",
      "70 batch_normalization_24\n",
      "71 activation_119\n",
      "72 activation_122\n",
      "73 average_pooling2d_3\n",
      "74 conv2d_20\n",
      "75 conv2d_22\n",
      "76 conv2d_25\n",
      "77 conv2d_26\n",
      "78 batch_normalization_20\n",
      "79 batch_normalization_22\n",
      "80 batch_normalization_25\n",
      "81 batch_normalization_26\n",
      "82 activation_118\n",
      "83 activation_120\n",
      "84 activation_123\n",
      "85 activation_124\n",
      "86 mixed2\n",
      "87 conv2d_28\n",
      "88 batch_normalization_28\n",
      "89 activation_126\n",
      "90 conv2d_29\n",
      "91 batch_normalization_29\n",
      "92 activation_127\n",
      "93 conv2d_27\n",
      "94 conv2d_30\n",
      "95 batch_normalization_27\n",
      "96 batch_normalization_30\n",
      "97 activation_125\n",
      "98 activation_128\n",
      "99 max_pooling2d_5\n",
      "100 mixed3\n",
      "101 conv2d_35\n",
      "102 batch_normalization_35\n",
      "103 activation_133\n",
      "104 conv2d_36\n",
      "105 batch_normalization_36\n",
      "106 activation_134\n",
      "107 conv2d_32\n",
      "108 conv2d_37\n",
      "109 batch_normalization_32\n",
      "110 batch_normalization_37\n",
      "111 activation_130\n",
      "112 activation_135\n",
      "113 conv2d_33\n",
      "114 conv2d_38\n",
      "115 batch_normalization_33\n",
      "116 batch_normalization_38\n",
      "117 activation_131\n",
      "118 activation_136\n",
      "119 average_pooling2d_4\n",
      "120 conv2d_31\n",
      "121 conv2d_34\n",
      "122 conv2d_39\n",
      "123 conv2d_40\n",
      "124 batch_normalization_31\n",
      "125 batch_normalization_34\n",
      "126 batch_normalization_39\n",
      "127 batch_normalization_40\n",
      "128 activation_129\n",
      "129 activation_132\n",
      "130 activation_137\n",
      "131 activation_138\n",
      "132 mixed4\n",
      "133 conv2d_45\n",
      "134 batch_normalization_45\n",
      "135 activation_143\n",
      "136 conv2d_46\n",
      "137 batch_normalization_46\n",
      "138 activation_144\n",
      "139 conv2d_42\n",
      "140 conv2d_47\n",
      "141 batch_normalization_42\n",
      "142 batch_normalization_47\n",
      "143 activation_140\n",
      "144 activation_145\n",
      "145 conv2d_43\n",
      "146 conv2d_48\n",
      "147 batch_normalization_43\n",
      "148 batch_normalization_48\n",
      "149 activation_141\n",
      "150 activation_146\n",
      "151 average_pooling2d_5\n",
      "152 conv2d_41\n",
      "153 conv2d_44\n",
      "154 conv2d_49\n",
      "155 conv2d_50\n",
      "156 batch_normalization_41\n",
      "157 batch_normalization_44\n",
      "158 batch_normalization_49\n",
      "159 batch_normalization_50\n",
      "160 activation_139\n",
      "161 activation_142\n",
      "162 activation_147\n",
      "163 activation_148\n",
      "164 mixed5\n",
      "165 conv2d_55\n",
      "166 batch_normalization_55\n",
      "167 activation_153\n",
      "168 conv2d_56\n",
      "169 batch_normalization_56\n",
      "170 activation_154\n",
      "171 conv2d_52\n",
      "172 conv2d_57\n",
      "173 batch_normalization_52\n",
      "174 batch_normalization_57\n",
      "175 activation_150\n",
      "176 activation_155\n",
      "177 conv2d_53\n",
      "178 conv2d_58\n",
      "179 batch_normalization_53\n",
      "180 batch_normalization_58\n",
      "181 activation_151\n",
      "182 activation_156\n",
      "183 average_pooling2d_6\n",
      "184 conv2d_51\n",
      "185 conv2d_54\n",
      "186 conv2d_59\n",
      "187 conv2d_60\n",
      "188 batch_normalization_51\n",
      "189 batch_normalization_54\n",
      "190 batch_normalization_59\n",
      "191 batch_normalization_60\n",
      "192 activation_149\n",
      "193 activation_152\n",
      "194 activation_157\n",
      "195 activation_158\n",
      "196 mixed6\n",
      "197 conv2d_65\n",
      "198 batch_normalization_65\n",
      "199 activation_163\n",
      "200 conv2d_66\n",
      "201 batch_normalization_66\n",
      "202 activation_164\n",
      "203 conv2d_62\n",
      "204 conv2d_67\n",
      "205 batch_normalization_62\n",
      "206 batch_normalization_67\n",
      "207 activation_160\n",
      "208 activation_165\n",
      "209 conv2d_63\n",
      "210 conv2d_68\n",
      "211 batch_normalization_63\n",
      "212 batch_normalization_68\n",
      "213 activation_161\n",
      "214 activation_166\n",
      "215 average_pooling2d_7\n",
      "216 conv2d_61\n",
      "217 conv2d_64\n",
      "218 conv2d_69\n",
      "219 conv2d_70\n",
      "220 batch_normalization_61\n",
      "221 batch_normalization_64\n",
      "222 batch_normalization_69\n",
      "223 batch_normalization_70\n",
      "224 activation_159\n",
      "225 activation_162\n",
      "226 activation_167\n",
      "227 activation_168\n",
      "228 mixed7\n",
      "229 conv2d_73\n",
      "230 batch_normalization_73\n",
      "231 activation_171\n",
      "232 conv2d_74\n",
      "233 batch_normalization_74\n",
      "234 activation_172\n",
      "235 conv2d_71\n",
      "236 conv2d_75\n",
      "237 batch_normalization_71\n",
      "238 batch_normalization_75\n",
      "239 activation_169\n",
      "240 activation_173\n",
      "241 conv2d_72\n",
      "242 conv2d_76\n",
      "243 batch_normalization_72\n",
      "244 batch_normalization_76\n",
      "245 activation_170\n",
      "246 activation_174\n",
      "247 max_pooling2d_6\n",
      "248 mixed8\n",
      "249 conv2d_81\n",
      "250 batch_normalization_81\n",
      "251 activation_179\n",
      "252 conv2d_78\n",
      "253 conv2d_82\n",
      "254 batch_normalization_78\n",
      "255 batch_normalization_82\n",
      "256 activation_176\n",
      "257 activation_180\n",
      "258 conv2d_79\n",
      "259 conv2d_80\n",
      "260 conv2d_83\n",
      "261 conv2d_84\n",
      "262 average_pooling2d_8\n",
      "263 conv2d_77\n",
      "264 batch_normalization_79\n",
      "265 batch_normalization_80\n",
      "266 batch_normalization_83\n",
      "267 batch_normalization_84\n",
      "268 conv2d_85\n",
      "269 batch_normalization_77\n",
      "270 activation_177\n",
      "271 activation_178\n",
      "272 activation_181\n",
      "273 activation_182\n",
      "274 batch_normalization_85\n",
      "275 activation_175\n",
      "276 mixed9_0\n",
      "277 concatenate_1\n",
      "278 activation_183\n",
      "279 mixed9\n",
      "280 conv2d_90\n",
      "281 batch_normalization_90\n",
      "282 activation_188\n",
      "283 conv2d_87\n",
      "284 conv2d_91\n",
      "285 batch_normalization_87\n",
      "286 batch_normalization_91\n",
      "287 activation_185\n",
      "288 activation_189\n",
      "289 conv2d_88\n",
      "290 conv2d_89\n",
      "291 conv2d_92\n",
      "292 conv2d_93\n",
      "293 average_pooling2d_9\n",
      "294 conv2d_86\n",
      "295 batch_normalization_88\n",
      "296 batch_normalization_89\n",
      "297 batch_normalization_92\n",
      "298 batch_normalization_93\n",
      "299 conv2d_94\n",
      "300 batch_normalization_86\n",
      "301 activation_186\n",
      "302 activation_187\n",
      "303 activation_190\n",
      "304 activation_191\n",
      "305 batch_normalization_94\n",
      "306 activation_184\n",
      "307 mixed9_1\n",
      "308 concatenate_2\n",
      "309 activation_192\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "# model.fit_generator(...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other Examples\n",
    "\n",
    "https://github.com/keras-team/keras/tree/master/examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Project suggestions:\n",
    "\n",
    "* Image noice cleaning - сравнение на различни модели.\n",
    "* AutoML - генериране на модели.\n",
    "* Генериране на текстове за новини на български по зададени ключови думи. Генериране на текствое за песни.\n",
    "* Генериране на музика. Генериране на изображения (style transfer)\n",
    "* Spell, grammatical checker.\n",
    "* Document summarisation.\n",
    "* Nice visualization projects.\n",
    "* Data extraction.\n",
    "* Kaggle Competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
